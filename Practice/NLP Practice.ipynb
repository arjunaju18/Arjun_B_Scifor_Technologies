{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46a120a-da50-4817-b802-4b040bc36905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce7232-bfbb-45f3-9ab1-91311eb2ab01",
   "metadata": {},
   "source": [
    "## Strings are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b14dea6-edec-45f2-a5e6-0b74bc691207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Virat'\n",
    "a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292b31a-e17e-4d72-86a5-2bc7cc3a664a",
   "metadata": {},
   "source": [
    "## Slicing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95fbd807-ff3e-45aa-b809-c0ce3f901201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India won\n",
      "t20 \n"
     ]
    }
   ],
   "source": [
    "x = 'India won the t20 Worldcup'\n",
    "print(x[:9])\n",
    "\n",
    "print(x[-12:-8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c326529-bb73-49de-b616-2fc0156004f8",
   "metadata": {},
   "source": [
    "# String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8d32b3c-5a28-4f57-85f6-7e79ca0f6b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deutschland'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'Deutschland'\n",
    "x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c12d23c9-8679-4922-af95-501573a1767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEUTSCHLAND'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbaaea01-2d3e-4e08-b77c-4a3846246cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.replace('Deutschland', 'Germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448b212-9962-48dc-9214-ee69d0e1beed",
   "metadata": {},
   "source": [
    "## String Concatination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "618053dc-9d7a-4bca-8cdd-59e481d0ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli\n"
     ]
    }
   ],
   "source": [
    "a = 'Virat'\n",
    "b = \" Kohli\"\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77749666-c787-4595-a369-aa2c237f84b5",
   "metadata": {},
   "source": [
    "# Creating a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcb55cb2-2240-46f1-ad55-80b4630aef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"datatest.txt\", \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3ec598b-d3eb-4f19-a848-4af3cb7610de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  file1.write('line number is %d\\r\\n' %(i+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d39cdb9-727f-4fa0-9451-cd05c1a6d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df95e77-5e63-497e-81c9-8a8c3bb88fa1",
   "metadata": {},
   "source": [
    "## Appending Data To a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b34ab677-5075-42c6-b43d-3a470bfe4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('dataset.txt', 'a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04682eea-cbab-4fc2-95fa-238f59a46a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  file1.write('appending line number %d\\r\\n' %(i+1))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d3eee28-eb95-4836-950c-746858110b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8505f-127c-4c3b-afa2-e2697e8150d7",
   "metadata": {},
   "source": [
    "## Reading the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87c9941b-c15a-4a0b-850c-39d3cc49e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('dataset.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0be315ae-1e42-4911-8fa0-63438ab66afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file1.mode =='r':\n",
    "  content=file1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb6844b5-84f0-419a-85a6-7a0114972f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appending line number 1\n",
      "\n",
      "appending line number 2\n",
      "\n",
      "appending line number 3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896db6c0-887b-405c-856c-92aefb48e6f7",
   "metadata": {},
   "source": [
    "## removing numbers using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1027ecfb-3cd6-40e4-93fb-df2cdf5d9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    result = re.sub(r'\\d+','', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97eb0368-12fe-4f59-b15c-cd102145d461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_s =\"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(input_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0351244-5b65-4987-b98d-5863ddf2cbf4",
   "metadata": {},
   "source": [
    "## Using inflect library to convert numbers into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31c61b7-e0da-48c4-939f-7df0ba57f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "def convert_num(text):\n",
    "  temp_string = text.split()\n",
    "  new_str = []\n",
    "\n",
    "  for word in temp_string:\n",
    "    if word.isdigit():\n",
    "      temp = p.number_to_words(word)\n",
    "      new_str.append(temp)\n",
    "    else:\n",
    "      new_str.append(word)\n",
    "\n",
    "  temp_string = ' '.join(new_str)\n",
    "  return temp_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21971698-1661-4a18-8ad5-729f87799d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you bought ten candies from shop, and five candies are in home\n"
     ]
    }
   ],
   "source": [
    "input_str = 'you bought 10 candies from shop, and 5 candies are in home'\n",
    "print(convert_num(input_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bcb7ef-d0a2-492e-a8c6-7e63f7c265d4",
   "metadata": {},
   "source": [
    "# Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9ef471-8617-4176-b2a8-564ef1e3154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862883bd-a482-45eb-ba82-cf26a3babc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey are you exited  after a week  we will be in shimla '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = 'hey, are you exited !!!, after a week , we will be in shimla !!!'\n",
    "rem_punc(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a41436-b0ed-4d65-ac6a-f88858f15ee5",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4e2d8f-d61d-4c31-8975-e40f0e2db444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c52a542-c88e-450f-ae64-367780c25be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_stopwords(text):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  word_tokens = word_tokenize(text)\n",
    "  filtered_text = [word for word in word_tokens if word not in string.punctuation]\n",
    "  filtered_text = [word for word in filtered_text if word not in stop_words]\n",
    "  return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231bca4d-eab8-46d3-98ae-d5184cb3943d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People', 'lazy', 'nowadays', 'Try', 'productive']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'People are being lazy nowadays. Try to be more productive.'\n",
    "rem_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c3104-96fc-4700-bdd9-8f215b03553f",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86cca2cb-e80e-40c9-bd7c-0bae4b7da0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a3b8622-5313-43ba-abf4-451c3f2feb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  stem = [stem1.stem(word) for word in word_tokens]\n",
    "  return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7280fcbe-3420-4acf-8a95-d75ba2abe450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peopl',\n",
       " 'are',\n",
       " 'be',\n",
       " 'lazi',\n",
       " 'nowaday',\n",
       " '.',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'be',\n",
       " 'more',\n",
       " 'product',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'People are being lazy nowadays. Try to be more productive.'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89e2bb-8c61-4bd5-ac2d-51a5316ca7d6",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12923071-9207-4141-8b75-dcc590226d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ajupo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9355f4ed-1bff-430c-bae3-c143d699f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemma.lemmatize(word, pos ='v')for word in word_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1278e4a3-775a-4b45-8800-ebb04d575686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'od',\n",
       " 'data']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'data is the new revolution in the world , in a day one individual would generate terabytes od data'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c126d-5f31-4e70-beb2-bb18a8e8d910",
   "metadata": {},
   "source": [
    "# Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02da5cdb-f42b-48bd-9e98-6f94f50ad788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13264088-2819-4573-8faf-061c974afb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Why', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('excited', 'VBD'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# convert text into word tokens with their tags\n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "# Example usage\n",
    "pos_tagg(\"Why are you so excited?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102e192-f115-4fef-80b1-4392e9539e62",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab2fb892-f147-495f-a0c8-75c588f3ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8272dc3e-9873-47f4-a94d-a808ccaa5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "def chunking(text, grammar):\n",
    "  word_tokens = word_tokenize(text)\n",
    "\n",
    "  # label words with pos\n",
    "  word_pos = pos_tag(word_tokens)\n",
    "\n",
    "  # create chunk parser using grammar\n",
    "  chunkParser = nltk.RegexpParser(grammar)\n",
    "\n",
    "  # test it on the list of word tokens with tagged pos\n",
    "  tree = chunkParser.parse(word_pos)\n",
    "\n",
    "  # iterate over the parse tree and print subtrees\n",
    "  for subtree in tree.subtrees():\n",
    "    print(subtree)\n",
    "\n",
    "# sentence to be chunked\n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "\n",
    "# Regular expression grammar for Noun Phrase (NP)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Calling the chunking function with the sentence and grammar\n",
    "chunking(sentence, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb06751-cea2-46ab-a888-b1c8a86fde5e",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efff3680-9f8b-4385-a72f-d5610e1c6a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Ajupo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ajupo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f524dd2-ae79-4b82-97ac-6c271cbccf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def ner(text):                    #tokenize the text\n",
    "  word_tokens = word_tokenize(text)\n",
    "\n",
    "  #pos tagging of words\n",
    "  word_pos = pos_tag(word_tokens)\n",
    "\n",
    "  #tree of word entities\n",
    "  print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
    "ner(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a29648-9287-4e9f-a738-c05c3e67e6e7",
   "metadata": {},
   "source": [
    "# Understanding Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bda69303-4d5d-4b3f-9f84-fe01a3bf87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent = \"dataset, Data is a new fuel\"\n",
    "r2 = re.findall(r\"^\\w+\", sent)\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced49bb-ebb6-4c92-b9c5-634ea526c4c6",
   "metadata": {},
   "source": [
    "# re.split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e780f535-de95-4ffd-a227-6e017a49fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'Data', 'is', 'a', 'new', 'fuel']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent = \"dataset, Data is a new fuel\"\n",
    "r2 = re.findall(r\"\\w+\", sent)  # Find all word characters at the beginning of the string\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f77759-666b-4b06-9977-a034ab7770fc",
   "metadata": {},
   "source": [
    "# re.match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3330f4c-8cf4-43b9-a240-10ebf73677fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('icecream', 'images')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lists = ['icecream images', 'i immitated', 'inner peace']\n",
    "\n",
    "for i in lists:\n",
    "    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
    "\n",
    "    if q:\n",
    "        print(q.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf7220-c885-45ed-98f9-7b6a6bae74af",
   "metadata": {},
   "source": [
    "# re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eeba46f3-a965-403d-8453-a11a9bba670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're looking for 'playing' in 'Raju is playing outside.'Found match!\n",
      "You're looking for 'dataset' in 'Raju is playing outside.'no match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = [\"playing\", \"dataset\"]\n",
    "text = \"Raju is playing outside.\"\n",
    "\n",
    "for p in pattern:\n",
    "    print(\"You're looking for '%s' in '%s'\" % (p, text), end='')\n",
    "\n",
    "    if re.search(p, text):\n",
    "        print('Found match!')\n",
    "    else:\n",
    "        print(\"no match found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd2738-c04d-4ffa-a32e-b22ba39a45f4",
   "metadata": {},
   "source": [
    "# re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17bfa8c4-a681-411c-8cb8-6b1cace2e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XYX@gmail.com\n",
      " lmn@gmail.com\n",
      " efg@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "kgf = \"Abc@gmail.com, XYX@gmail.com, lmn@gmail.com, efg@gmail.com\"\n",
    "emails = re.findall(r' [\\w\\.]+@[\\w\\.]+', kgf)\n",
    "\n",
    "for e in emails:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff29e6-34a8-4afa-871f-2295e8118d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
